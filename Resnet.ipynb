{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os \n",
    "import numpy as np\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.preprocessing import image\n",
    "from tensorflow.keras.applications import ResNet50\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "import os\n",
    "from PIL import Image\n",
    "from os import listdir\n",
    "from tqdm import tqdm\n",
    "import shutil\n",
    "import cv2\n",
    "%matplotlib inline\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "ORIG_INPUT_DATASET = \"baris/O/\"\n",
    "BASE_PATH = \"dataset\"\n",
    "\n",
    "# Load in the images\n",
    "instances = []\n",
    "for filepath in os.listdir(ORIG_INPUT_DATASET):\n",
    "    instances.append(cv2.imread(\"baris/O/{}\".format(filepath)))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extracting only faces of objects using cascade classifier "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "face_cascade = cv2.CascadeClassifier('haarcascade_frontalface_default.xml')\n",
    "count = 0\n",
    "path = 'baris/croped'\n",
    "for i in instances:\n",
    "    gray = cv2.cvtColor(i, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "    face = face_cascade.detectMultiScale(gray, scaleFactor=1.5, minNeighbors=5)\n",
    "\n",
    "    for (x,y,w,h) in face:\n",
    "        cv2.rectangle(face, (x,y), (x+w,y+h), (255,0,0), 2)     \n",
    "        count += 1\n",
    "        # Save the captured image into the datasets folder\n",
    "        cv2.imwrite(str(path) +'/' + str(count) + \".jpg\", i[y:y+h,x:x+w])\n",
    "\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "count = 0\n",
    "face_path = 'baris/croped'\n",
    "resized_IMG_path = 'baris/resized/'\n",
    "for img  in os.listdir(face_path):\n",
    "    count += 1\n",
    "    file = Image.open(\"baris/croped/{}\".format(img))\n",
    "    img_inter = file.resize((250, 250))\n",
    "    img_inter.save('baris/resized/{}.jpg'.format(count))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "TRAIN = \"training\"\n",
    "TEST = \"evaluation\"\n",
    "VAL = \"validation\"\n",
    "\n",
    "BASE_PATH = \"dataset\"\n",
    "BATCH_SIZE = 32\n",
    "CLASSES = [\"0\", \"1\"] # \"Neutral\", "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I labeled 246 images as target values manually and saved them in labeles.txt file. \n",
    "Note: I want to detect Baris Ozcan's (a famous Youtuber) face while he is talking in his videos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "246"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with open(\"labels.txt\", 'r') as f:\n",
    "    manual_labels = f.read()\n",
    "\n",
    "manual_labels = manual_labels.replace(',', '')    \n",
    "labels = [i for i in manual_labels]\n",
    "len(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Important to sort the files in the order in which we see them in folder\n",
    "directory = \"data\"\n",
    "files = os.listdir(directory)\n",
    "files.sort(key=lambda f: int(f.split('.')[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainX, testX, trainY, testY =  train_test_split(files[:len(labels)], labels, stratify=labels, train_size=0.9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# further splitting of train set into train and val sets\n",
    "trainX, valX, trainY, valY = train_test_split(trainX, trainY, stratify=trainY, train_size=0.85)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(187, 187, 34, 34, 25, 25)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(trainX), len(trainY), len(valX), len(valY),  len(testX), len(testY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Iterating over images in sub folder: 100%|██████████| 187/187 [00:03<00:00, 53.65it/s]\n",
      "Iterating over images in sub folder: 100%|██████████| 25/25 [00:00<00:00, 39.06it/s]\n",
      "Iterating over images in sub folder: 100%|██████████| 34/34 [00:00<00:00, 54.67it/s]\n"
     ]
    }
   ],
   "source": [
    "# Building the dataset properly - \n",
    "splits = [(trainX, trainY), (testX, testY), (valX, valY)]\n",
    "dirnames = ['training', 'evaluation', 'validation']\n",
    "\n",
    "for i, (data,label) in enumerate(splits):\n",
    "    outside_dir=dirnames[i]\n",
    "\n",
    "    for j in tqdm(range(0, len(label)), desc=\"Iterating over images in sub folder\"):\n",
    "        dir = label[j]\n",
    "        \n",
    "        # construct the path to the sub-directory\n",
    "        dirPath = os.path.join(BASE_PATH, outside_dir, dir)\n",
    "        \n",
    "        # if the output directory does not exist, create it\n",
    "        if not os.path.exists(dirPath):\n",
    "            os.makedirs(dirPath)\n",
    "            \n",
    "            \n",
    "        # copy the img to this new directory\n",
    "        src_img = os.path.join(\"data\", data[j])\n",
    "        shutil.copy(src_img, dirPath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# derive the paths to the training, validation, and testing directories\n",
    "trainPath = os.path.sep.join([BASE_PATH, TRAIN])\n",
    "valPath = os.path.sep.join([BASE_PATH, VAL])\n",
    "testPath = os.path.sep.join([BASE_PATH, TEST])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# determine the total number of image paths in training, validation,\n",
    "# and testing directories\n",
    "from imutils import paths\n",
    "totalTrain = len(list(paths.list_images(trainPath)))\n",
    "totalVal = len(list(paths.list_images(valPath)))\n",
    "totalTest = len(list(paths.list_images(testPath)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "187 25 34\n"
     ]
    }
   ],
   "source": [
    "print(totalTrain, totalTest, totalVal)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize the training data augmentation object\n",
    "trainAug = ImageDataGenerator(\n",
    "\trotation_range=90,\n",
    "\tzoom_range=[0.5, 1.0],\n",
    "\twidth_shift_range=0.3,\n",
    "\theight_shift_range=0.25,\n",
    "\tshear_range=0.15,\n",
    "\thorizontal_flip=True,\n",
    "\tfill_mode=\"nearest\",\n",
    " \tbrightness_range=[0.2, 1.0]\n",
    "   )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Default for all the above parameters is 0, \n",
    "# meaning we are applying no augmentation to val set\n",
    "# which is exactly what we need because val set should be treated like test set.\n",
    "valAug = ImageDataGenerator()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "testAug = ImageDataGenerator()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 187 images belonging to 2 classes.\n"
     ]
    }
   ],
   "source": [
    "# Create batches whilst creating augmented images on the fly\n",
    "\n",
    "trainGen = trainAug.flow_from_directory(\n",
    "    directory=trainPath,\n",
    "    target_size=(250,250),\n",
    "    save_to_dir='dataset/augmented/train',\n",
    "    save_prefix='train',\n",
    "    shuffle=True # data will be shuffled between epochs\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 34 images belonging to 2 classes.\n"
     ]
    }
   ],
   "source": [
    "valGen = valAug.flow_from_directory(\n",
    "    directory=valPath,\n",
    "    target_size=(250,250),\n",
    "    shuffle=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 25 images belonging to 2 classes.\n"
     ]
    }
   ],
   "source": [
    "testGen = testAug.flow_from_directory(\n",
    "    directory=testPath,\n",
    "    target_size=(250,250),\n",
    "    shuffle=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_height, img_width = 250, 250\n",
    "\n",
    "base_model = ResNet50(weights = 'imagenet',\n",
    "                      include_top = False, \n",
    "                      input_shape = (img_height, img_width, 3))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_without_top = ResNet50(weights = 'imagenet',\n",
    "                             include_top = False, \n",
    "                             input_shape = (img_height, img_width, 3))\n",
    "\n",
    "                              \n",
    "model_with_top = ResNet50(weights= 'imagenet',\n",
    "                          include_top = True, \n",
    "                          input_shape = (224, 224, 3))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_without_top.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_with_top.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "classes = ['0', '1']\n",
    "num_classes = len(classes)\n",
    "\n",
    "for layer in base_model.layers:\n",
    "    layer.trainable = False\n",
    "\n",
    "global_average_pooling = keras.layers.GlobalAveragePooling2D()(base_model.output)\n",
    "output = keras.layers.Dense(num_classes, activation = 'sigmoid')(global_average_pooling)\n",
    "\n",
    "face_classifier = keras.models.Model(inputs = base_model.input, outputs = output, name = 'ResNet50')\n",
    "\n",
    "# saving classifier \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ModelCheckpoint to save model in case of interrupting the learning process\n",
    "checkpoint = ModelCheckpoint(\"face_classifier.h5\",\n",
    "                             monitor=\"val_loss\",\n",
    "                             mode=\"min\",\n",
    "                             save_best_only=True,\n",
    "                             verbose=1)\n",
    "\n",
    "# EarlyStopping to find best model with a large number of epochs\n",
    "earlystop = EarlyStopping(monitor='val_loss',\n",
    "                          restore_best_weights=True,\n",
    "                          patience=3,  # number of epochs with no improvement after which training will be stopped\n",
    "                          verbose=1)\n",
    "\n",
    "callbacks = [earlystop, checkpoint]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "face_classifier.compile(loss='categorical_crossentropy',\n",
    "                        optimizer=keras.optimizers.Adam(learning_rate=0.01),\n",
    "                       metrics=[tf.keras.metrics.AUC()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 2.27330, saving model to face_classifier.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\seyda\\anaconda3\\envs\\kt\\lib\\site-packages\\keras\\engine\\functional.py:1410: CustomMaskWarning: Custom mask layers require a config and must override get_config. When loading, the custom mask layer must be passed to the custom_objects argument.\n",
      "  layer_config = serialize_layer_fn(layer)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5/5 - 139s - loss: 1.6692 - auc: 0.7602 - val_loss: 2.2733 - val_auc: 0.8374 - 139s/epoch - 28s/step\n",
      "Epoch 2/50\n",
      "\n",
      "Epoch 00002: val_loss improved from 2.27330 to 0.96940, saving model to face_classifier.h5\n",
      "5/5 - 121s - loss: 1.0389 - auc: 0.8971 - val_loss: 0.9694 - val_auc: 0.7444 - 121s/epoch - 24s/step\n",
      "Epoch 3/50\n",
      "\n",
      "Epoch 00003: val_loss improved from 0.96940 to 0.59899, saving model to face_classifier.h5\n",
      "5/5 - 125s - loss: 0.6154 - auc: 0.9056 - val_loss: 0.5990 - val_auc: 0.9459 - 125s/epoch - 25s/step\n",
      "Epoch 4/50\n",
      "\n",
      "Epoch 00004: val_loss improved from 0.59899 to 0.36213, saving model to face_classifier.h5\n",
      "5/5 - 137s - loss: 0.6507 - auc: 0.9499 - val_loss: 0.3621 - val_auc: 0.9706 - 137s/epoch - 27s/step\n",
      "Epoch 5/50\n",
      "\n",
      "Epoch 00005: val_loss did not improve from 0.36213\n",
      "5/5 - 122s - loss: 0.2001 - auc: 0.9876 - val_loss: 0.4170 - val_auc: 0.9650 - 122s/epoch - 24s/step\n",
      "Epoch 6/50\n",
      "\n",
      "Epoch 00006: val_loss improved from 0.36213 to 0.26009, saving model to face_classifier.h5\n",
      "5/5 - 119s - loss: 0.3263 - auc: 0.9701 - val_loss: 0.2601 - val_auc: 0.9784 - 119s/epoch - 24s/step\n",
      "Epoch 7/50\n",
      "\n",
      "Epoch 00007: val_loss did not improve from 0.26009\n",
      "5/5 - 129s - loss: 0.1826 - auc: 0.9892 - val_loss: 0.2760 - val_auc: 0.9766 - 129s/epoch - 26s/step\n",
      "Epoch 8/50\n",
      "\n",
      "Epoch 00008: val_loss did not improve from 0.26009\n",
      "5/5 - 119s - loss: 0.1061 - auc: 0.9949 - val_loss: 0.3012 - val_auc: 0.9706 - 119s/epoch - 24s/step\n",
      "Epoch 9/50\n",
      "Restoring model weights from the end of the best epoch: 6.\n",
      "\n",
      "Epoch 00009: val_loss did not improve from 0.26009\n",
      "5/5 - 124s - loss: 0.1712 - auc: 0.9861 - val_loss: 0.4498 - val_auc: 0.9481 - 124s/epoch - 25s/step\n",
      "Epoch 00009: early stopping\n"
     ]
    }
   ],
   "source": [
    "# Training the model\n",
    "hist = face_classifier.fit(\n",
    "    x=trainGen,\n",
    "    epochs = 50,\n",
    "    verbose=2,\n",
    "    validation_data=valGen,\n",
    "    # shuffle=True, # argument is ignored when `x` is a generator\n",
    "    steps_per_epoch=totalTrain // BATCH_SIZE,\n",
    "    callbacks = [earlystop, checkpoint]\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "b90776be2c57f72028669c84b7e1a9ce80ccfac1d8e7f8966ec485b1a61088ac"
  },
  "kernelspec": {
   "display_name": "Python 3.9.7 ('kt')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
